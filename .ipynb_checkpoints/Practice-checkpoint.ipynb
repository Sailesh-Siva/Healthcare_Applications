{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5cfd7b-3e75-4074-839f-87cb0af7f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "Python is an amazing programming language. It is used in data science, machine learning, web development,\n",
    "automation, and more. Python is easy to learn and powerful.\n",
    "\"\"\"\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.title(\"Word Cloud Example\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd03ed9f-6a68-483a-9ab8-9d0a20a46b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e14051-a94d-4451-8d4b-ca71dc2f6a29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARLSTem',\n",
       " 'ARLSTem2',\n",
       " 'AbstractLazySequence',\n",
       " 'AffixTagger',\n",
       " 'AlignedSent',\n",
       " 'Alignment',\n",
       " 'AnnotationTask',\n",
       " 'ApplicationExpression',\n",
       " 'Assignment',\n",
       " 'BigramAssocMeasures',\n",
       " 'BigramCollocationFinder',\n",
       " 'BigramTagger',\n",
       " 'BinaryMaxentFeatureEncoding',\n",
       " 'BlanklineTokenizer',\n",
       " 'BllipParser',\n",
       " 'BottomUpChartParser',\n",
       " 'BottomUpLeftCornerChartParser',\n",
       " 'BottomUpProbabilisticChartParser',\n",
       " 'Boxer',\n",
       " 'BrillTagger',\n",
       " 'BrillTaggerTrainer',\n",
       " 'CFG',\n",
       " 'CRFTagger',\n",
       " 'CfgReadingCommand',\n",
       " 'ChartParser',\n",
       " 'ChunkParserI',\n",
       " 'ChunkScore',\n",
       " 'Cistem',\n",
       " 'ClassifierBasedPOSTagger',\n",
       " 'ClassifierBasedTagger',\n",
       " 'ClassifierI',\n",
       " 'ConcordanceIndex',\n",
       " 'ConditionalExponentialClassifier',\n",
       " 'ConditionalFreqDist',\n",
       " 'ConditionalProbDist',\n",
       " 'ConditionalProbDistI',\n",
       " 'ConfusionMatrix',\n",
       " 'ContextIndex',\n",
       " 'ContextTagger',\n",
       " 'ContingencyMeasures',\n",
       " 'CoreNLPDependencyParser',\n",
       " 'CoreNLPParser',\n",
       " 'CrossValidationProbDist',\n",
       " 'DRS',\n",
       " 'DecisionTreeClassifier',\n",
       " 'DefaultTagger',\n",
       " 'DependencyEvaluator',\n",
       " 'DependencyGrammar',\n",
       " 'DependencyGraph',\n",
       " 'DependencyProduction',\n",
       " 'DictionaryConditionalProbDist',\n",
       " 'DictionaryProbDist',\n",
       " 'DiscourseTester',\n",
       " 'DrtExpression',\n",
       " 'DrtGlueReadingCommand',\n",
       " 'ELEProbDist',\n",
       " 'EarleyChartParser',\n",
       " 'Expression',\n",
       " 'FStructure',\n",
       " 'FeatDict',\n",
       " 'FeatList',\n",
       " 'FeatStruct',\n",
       " 'FeatStructReader',\n",
       " 'Feature',\n",
       " 'FeatureBottomUpChartParser',\n",
       " 'FeatureBottomUpLeftCornerChartParser',\n",
       " 'FeatureChartParser',\n",
       " 'FeatureEarleyChartParser',\n",
       " 'FeatureIncrementalBottomUpChartParser',\n",
       " 'FeatureIncrementalBottomUpLeftCornerChartParser',\n",
       " 'FeatureIncrementalChartParser',\n",
       " 'FeatureIncrementalTopDownChartParser',\n",
       " 'FeatureTopDownChartParser',\n",
       " 'FreqDist',\n",
       " 'HTTPPasswordMgrWithDefaultRealm',\n",
       " 'HeldoutProbDist',\n",
       " 'HiddenMarkovModelTagger',\n",
       " 'HiddenMarkovModelTrainer',\n",
       " 'HunposTagger',\n",
       " 'IBMModel',\n",
       " 'IBMModel1',\n",
       " 'IBMModel2',\n",
       " 'IBMModel3',\n",
       " 'IBMModel4',\n",
       " 'IBMModel5',\n",
       " 'ISRIStemmer',\n",
       " 'ImmutableMultiParentedTree',\n",
       " 'ImmutableParentedTree',\n",
       " 'ImmutableProbabilisticMixIn',\n",
       " 'ImmutableProbabilisticTree',\n",
       " 'ImmutableTree',\n",
       " 'IncrementalBottomUpChartParser',\n",
       " 'IncrementalBottomUpLeftCornerChartParser',\n",
       " 'IncrementalChartParser',\n",
       " 'IncrementalLeftCornerChartParser',\n",
       " 'IncrementalTopDownChartParser',\n",
       " 'Index',\n",
       " 'InsideChartParser',\n",
       " 'JSONTaggedDecoder',\n",
       " 'JSONTaggedEncoder',\n",
       " 'KneserNeyProbDist',\n",
       " 'LancasterStemmer',\n",
       " 'LaplaceProbDist',\n",
       " 'LazyConcatenation',\n",
       " 'LazyEnumerate',\n",
       " 'LazyIteratorList',\n",
       " 'LazyMap',\n",
       " 'LazySubsequence',\n",
       " 'LazyZip',\n",
       " 'LeftCornerChartParser',\n",
       " 'LegalitySyllableTokenizer',\n",
       " 'LidstoneProbDist',\n",
       " 'LineTokenizer',\n",
       " 'LogicalExpressionException',\n",
       " 'LongestChartParser',\n",
       " 'MLEProbDist',\n",
       " 'MWETokenizer',\n",
       " 'Mace',\n",
       " 'MaceCommand',\n",
       " 'MaltParser',\n",
       " 'MaxentClassifier',\n",
       " 'Maxent_NE_Chunker',\n",
       " 'Model',\n",
       " 'MultiClassifierI',\n",
       " 'MultiParentedTree',\n",
       " 'MutableProbDist',\n",
       " 'NLTKWordTokenizer',\n",
       " 'NaiveBayesClassifier',\n",
       " 'NaiveBayesDependencyScorer',\n",
       " 'NgramAssocMeasures',\n",
       " 'NgramTagger',\n",
       " 'NonprojectiveDependencyParser',\n",
       " 'Nonterminal',\n",
       " 'OrderedDict',\n",
       " 'PCFG',\n",
       " 'PRETRAINED_TAGGERS',\n",
       " 'Paice',\n",
       " 'ParallelProverBuilder',\n",
       " 'ParallelProverBuilderCommand',\n",
       " 'ParentedTree',\n",
       " 'ParserI',\n",
       " 'PerceptronTagger',\n",
       " 'PhraseTable',\n",
       " 'PorterStemmer',\n",
       " 'PositiveNaiveBayesClassifier',\n",
       " 'ProbDistI',\n",
       " 'ProbabilisticDependencyGrammar',\n",
       " 'ProbabilisticMixIn',\n",
       " 'ProbabilisticNonprojectiveParser',\n",
       " 'ProbabilisticProduction',\n",
       " 'ProbabilisticProjectiveDependencyParser',\n",
       " 'ProbabilisticTree',\n",
       " 'Production',\n",
       " 'ProjectiveDependencyParser',\n",
       " 'Prover9',\n",
       " 'Prover9Command',\n",
       " 'ProxyBasicAuthHandler',\n",
       " 'ProxyDigestAuthHandler',\n",
       " 'ProxyHandler',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'PunktTokenizer',\n",
       " 'QuadgramAssocMeasures',\n",
       " 'QuadgramCollocationFinder',\n",
       " 'RSLPStemmer',\n",
       " 'RTEFeatureExtractor',\n",
       " 'RandomChartParser',\n",
       " 'RangeFeature',\n",
       " 'ReadingCommand',\n",
       " 'RecursiveDescentParser',\n",
       " 'RegexpChunkParser',\n",
       " 'RegexpParser',\n",
       " 'RegexpStemmer',\n",
       " 'RegexpTagger',\n",
       " 'RegexpTokenizer',\n",
       " 'ReppTokenizer',\n",
       " 'ResolutionProver',\n",
       " 'ResolutionProverCommand',\n",
       " 'SExprTokenizer',\n",
       " 'SLASH',\n",
       " 'Senna',\n",
       " 'SennaChunkTagger',\n",
       " 'SennaNERTagger',\n",
       " 'SennaTagger',\n",
       " 'SequentialBackoffTagger',\n",
       " 'ShiftReduceParser',\n",
       " 'SimpleGoodTuringProbDist',\n",
       " 'SklearnClassifier',\n",
       " 'SlashFeature',\n",
       " 'SnowballStemmer',\n",
       " 'SpaceTokenizer',\n",
       " 'StackDecoder',\n",
       " 'StanfordNERTagger',\n",
       " 'StanfordPOSTagger',\n",
       " 'StanfordSegmenter',\n",
       " 'StanfordTagger',\n",
       " 'StemmerI',\n",
       " 'SteppingChartParser',\n",
       " 'SteppingRecursiveDescentParser',\n",
       " 'SteppingShiftReduceParser',\n",
       " 'SyllableTokenizer',\n",
       " 'TYPE',\n",
       " 'TabTokenizer',\n",
       " 'TableauProver',\n",
       " 'TableauProverCommand',\n",
       " 'TaggerI',\n",
       " 'TestGrammar',\n",
       " 'Text',\n",
       " 'TextCat',\n",
       " 'TextCollection',\n",
       " 'TextTilingTokenizer',\n",
       " 'TnT',\n",
       " 'TokenSearcher',\n",
       " 'ToktokTokenizer',\n",
       " 'TopDownChartParser',\n",
       " 'TransitionParser',\n",
       " 'Tree',\n",
       " 'TreePrettyPrinter',\n",
       " 'TreebankWordDetokenizer',\n",
       " 'TreebankWordTokenizer',\n",
       " 'Trie',\n",
       " 'TrigramAssocMeasures',\n",
       " 'TrigramCollocationFinder',\n",
       " 'TrigramTagger',\n",
       " 'TweetTokenizer',\n",
       " 'TypedMaxentFeatureEncoding',\n",
       " 'Undefined',\n",
       " 'UniformProbDist',\n",
       " 'UnigramTagger',\n",
       " 'UnsortedChartParser',\n",
       " 'Valuation',\n",
       " 'Variable',\n",
       " 'ViterbiParser',\n",
       " 'WekaClassifier',\n",
       " 'WhitespaceTokenizer',\n",
       " 'WittenBellProbDist',\n",
       " 'WordNetLemmatizer',\n",
       " 'WordPunctTokenizer',\n",
       " '__author__',\n",
       " '__author_email__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__classifiers__',\n",
       " '__copyright__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__keywords__',\n",
       " '__license__',\n",
       " '__loader__',\n",
       " '__longdescr__',\n",
       " '__maintainer__',\n",
       " '__maintainer_email__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__url__',\n",
       " '__version__',\n",
       " 'accuracy',\n",
       " 'acyclic_branches_depth_first',\n",
       " 'acyclic_breadth_first',\n",
       " 'acyclic_depth_first',\n",
       " 'acyclic_dic2tree',\n",
       " 'add_logs',\n",
       " 'agreement',\n",
       " 'align',\n",
       " 'alignment_error_rate',\n",
       " 'aline',\n",
       " 'api',\n",
       " 'app',\n",
       " 'apply_features',\n",
       " 'approxrand',\n",
       " 'arity',\n",
       " 'arlstem',\n",
       " 'arlstem2',\n",
       " 'association',\n",
       " 'bigrams',\n",
       " 'binary_distance',\n",
       " 'binary_search_file',\n",
       " 'binding_ops',\n",
       " 'bisect',\n",
       " 'blankline_tokenize',\n",
       " 'bleu',\n",
       " 'bleu_score',\n",
       " 'bllip',\n",
       " 'boolean_ops',\n",
       " 'boxer',\n",
       " 'bracket_parse',\n",
       " 'breadth_first',\n",
       " 'brill',\n",
       " 'brill_trainer',\n",
       " 'build_opener',\n",
       " 'call_megam',\n",
       " 'casual',\n",
       " 'casual_tokenize',\n",
       " 'ccg',\n",
       " 'chain',\n",
       " 'chart',\n",
       " 'chat',\n",
       " 'chomsky_normal_form',\n",
       " 'choose',\n",
       " 'chrf',\n",
       " 'chrf_score',\n",
       " 'chunk',\n",
       " 'cistem',\n",
       " 'classify',\n",
       " 'clause',\n",
       " 'clean_html',\n",
       " 'clean_url',\n",
       " 'cluster',\n",
       " 'collapse_unary',\n",
       " 'collections',\n",
       " 'collocations',\n",
       " 'combinations',\n",
       " 'config_java',\n",
       " 'config_megam',\n",
       " 'config_weka',\n",
       " 'conflicts',\n",
       " 'confusionmatrix',\n",
       " 'conllstr2tree',\n",
       " 'conlltags2tree',\n",
       " 'corenlp',\n",
       " 'corpus',\n",
       " 'crf',\n",
       " 'custom_distance',\n",
       " 'cut_string',\n",
       " 'data',\n",
       " 'decisiontree',\n",
       " 'decorator',\n",
       " 'decorators',\n",
       " 'defaultdict',\n",
       " 'demo',\n",
       " 'dependencygraph',\n",
       " 'deprecated',\n",
       " 'deque',\n",
       " 'destructive',\n",
       " 'discourse',\n",
       " 'distance',\n",
       " 'download',\n",
       " 'download_gui',\n",
       " 'download_shell',\n",
       " 'downloader',\n",
       " 'draw',\n",
       " 'drt',\n",
       " 'earleychart',\n",
       " 'edge_closure',\n",
       " 'edges2dot',\n",
       " 'edit_distance',\n",
       " 'edit_distance_align',\n",
       " 'elementtree_indent',\n",
       " 'entropy',\n",
       " 'equality_preds',\n",
       " 'evaluate',\n",
       " 'evaluate_sents',\n",
       " 'everygrams',\n",
       " 'extract',\n",
       " 'extract_rels',\n",
       " 'extract_test_sentences',\n",
       " 'f_measure',\n",
       " 'featstruct',\n",
       " 'featurechart',\n",
       " 'filestring',\n",
       " 'find',\n",
       " 'flatten',\n",
       " 'fractional_presence',\n",
       " 'functools',\n",
       " 'gale_church',\n",
       " 'gdfa',\n",
       " 'getproxies',\n",
       " 'ghd',\n",
       " 'gleu',\n",
       " 'gleu_score',\n",
       " 'glue',\n",
       " 'grammar',\n",
       " 'grow_diag_final_and',\n",
       " 'guess_encoding',\n",
       " 'help',\n",
       " 'hmm',\n",
       " 'hunpos',\n",
       " 'ibm1',\n",
       " 'ibm2',\n",
       " 'ibm3',\n",
       " 'ibm4',\n",
       " 'ibm5',\n",
       " 'ibm_model',\n",
       " 'ieerstr2tree',\n",
       " 'in_idle',\n",
       " 'induce_pcfg',\n",
       " 'inference',\n",
       " 'infile',\n",
       " 'inspect',\n",
       " 'install_opener',\n",
       " 'internals',\n",
       " 'interpret_sents',\n",
       " 'interval_distance',\n",
       " 'invert_dict',\n",
       " 'invert_graph',\n",
       " 'is_rel',\n",
       " 'islice',\n",
       " 'isri',\n",
       " 'jaccard_distance',\n",
       " 'json_tags',\n",
       " 'jsontags',\n",
       " 'lancaster',\n",
       " 'lazyimport',\n",
       " 'legality_principle',\n",
       " 'lfg',\n",
       " 'line_tokenize',\n",
       " 'linearlogic',\n",
       " 'lm',\n",
       " 'load',\n",
       " 'load_parser',\n",
       " 'locale',\n",
       " 'log_likelihood',\n",
       " 'logic',\n",
       " 'mace',\n",
       " 'malt',\n",
       " 'map_tag',\n",
       " 'mapping',\n",
       " 'masi_distance',\n",
       " 'maxent',\n",
       " 'megam',\n",
       " 'memoize',\n",
       " 'meteor',\n",
       " 'meteor_score',\n",
       " 'metrics',\n",
       " 'misc',\n",
       " 'mwe',\n",
       " 'naivebayes',\n",
       " 'named_entity',\n",
       " 'ne_chunk',\n",
       " 'ne_chunk_sents',\n",
       " 'ne_chunker',\n",
       " 'ngrams',\n",
       " 'nist',\n",
       " 'nist_score',\n",
       " 'nonprojectivedependencyparser',\n",
       " 'nonterminals',\n",
       " 'numpy',\n",
       " 'os',\n",
       " 'pad_sequence',\n",
       " 'paice',\n",
       " 'pairwise',\n",
       " 'parallelize_preprocess',\n",
       " 'parse',\n",
       " 'parse_sents',\n",
       " 'pchart',\n",
       " 'perceptron',\n",
       " 'phrase_based',\n",
       " 'pk',\n",
       " 'porter',\n",
       " 'pos_tag',\n",
       " 'pos_tag_sents',\n",
       " 'positivenaivebayes',\n",
       " 'pprint',\n",
       " 'pr',\n",
       " 'precision',\n",
       " 'presence',\n",
       " 'print_string',\n",
       " 'probability',\n",
       " 'projectivedependencyparser',\n",
       " 'prover9',\n",
       " 'punkt',\n",
       " 'pydoc',\n",
       " 'raise_unorderable_types',\n",
       " 'ranks_from_scores',\n",
       " 'ranks_from_sequence',\n",
       " 're',\n",
       " 're_show',\n",
       " 'read_grammar',\n",
       " 'read_logic',\n",
       " 'read_valuation',\n",
       " 'recall',\n",
       " 'recursivedescent',\n",
       " 'regexp',\n",
       " 'regexp_span_tokenize',\n",
       " 'regexp_tokenize',\n",
       " 'register_tag',\n",
       " 'relextract',\n",
       " 'repp',\n",
       " 'resolution',\n",
       " 'ribes',\n",
       " 'ribes_score',\n",
       " 'root_semrep',\n",
       " 'rslp',\n",
       " 'rte_classifier',\n",
       " 'rte_classify',\n",
       " 'rte_features',\n",
       " 'rtuple',\n",
       " 'scikitlearn',\n",
       " 'scores',\n",
       " 'segmentation',\n",
       " 'sem',\n",
       " 'senna',\n",
       " 'sent_tokenize',\n",
       " 'sequential',\n",
       " 'set2rel',\n",
       " 'set_proxy',\n",
       " 'sexpr',\n",
       " 'sexpr_tokenize',\n",
       " 'shiftreduce',\n",
       " 'simple',\n",
       " 'sinica_parse',\n",
       " 'skipgrams',\n",
       " 'skolemize',\n",
       " 'slice_bounds',\n",
       " 'snowball',\n",
       " 'sonority_sequencing',\n",
       " 'spearman',\n",
       " 'spearman_correlation',\n",
       " 'stack_decoder',\n",
       " 'stanford',\n",
       " 'stanford_segmenter',\n",
       " 'stem',\n",
       " 'str2tuple',\n",
       " 'string_span_tokenize',\n",
       " 'subprocess',\n",
       " 'subsumes',\n",
       " 'sum_logs',\n",
       " 'tableau',\n",
       " 'tadm',\n",
       " 'tag',\n",
       " 'tagset_mapping',\n",
       " 'tagstr2tree',\n",
       " 'tbl',\n",
       " 'tee',\n",
       " 'text',\n",
       " 'textcat',\n",
       " 'texttiling',\n",
       " 'textwrap',\n",
       " 'tkinter',\n",
       " 'tnt',\n",
       " 'tokenize',\n",
       " 'tokenwrap',\n",
       " 'toktok',\n",
       " 'toolbox',\n",
       " 'total_ordering',\n",
       " 'trace',\n",
       " 'transitionparser',\n",
       " 'transitive_closure',\n",
       " 'translate',\n",
       " 'tree',\n",
       " 'tree2conllstr',\n",
       " 'tree2conlltags',\n",
       " 'treebank',\n",
       " 'trigrams',\n",
       " 'tuple2str',\n",
       " 'un_chomsky_normal_form',\n",
       " 'unicodedata',\n",
       " 'unify',\n",
       " 'unique_list',\n",
       " 'untag',\n",
       " 'unweighted_minimum_spanning_dict',\n",
       " 'unweighted_minimum_spanning_digraph',\n",
       " 'unweighted_minimum_spanning_tree',\n",
       " 'usage',\n",
       " 'util',\n",
       " 'version_file',\n",
       " 'viterbi',\n",
       " 'warnings',\n",
       " 'weka',\n",
       " 'windowdiff',\n",
       " 'word_tokenize',\n",
       " 'wordnet',\n",
       " 'wordpunct_tokenize',\n",
       " 'wsd']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e1c8913-a136-405f-9582-ff69f7f6126c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sailesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf2de732-f852-453b-9d6e-de6d97bdc0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Before you can begin to determine what the composition of a particular paragraph will be, you must first decide on an argument and a working thesis statement for your paper. What is the most important idea that you are trying to convey to your reader? The information in each paragraph must be related to that idea. In other words, your paragraphs should remind your reader that there is a recurrent relationship between your thesis and the information in each paragraph. A working thesis functions like a seed from which your paper, and your ideas, will grow. The whole process is an organic one—a natural progression from a seed to a full-blown paper where there are direct, familial relationships between all of the ideas in the paper.\n",
    "\n",
    "The decision about what to put into your paragraphs begins with the germination of a seed of ideas; this “germination process” is better known as brainstorming. There are many techniques for brainstorming; whichever one you choose, this stage of paragraph development cannot be skipped. Building paragraphs can be like building a skyscraper: there must be a well-planned foundation that supports what you are building. Any cracks, inconsistencies, or other corruptions of the foundation can cause your whole paper to crumble.\n",
    "\n",
    "So, let’s suppose that you have done some brainstorming to develop your thesis. What else should you keep in mind as you begin to create paragraphs? Every paragraph in a paper should be:\n",
    "\n",
    "Unified: All of the sentences in a single paragraph should be related to a single controlling idea (often expressed in the topic sentence of the paragraph).\n",
    "Clearly related to the thesis: The sentences should all refer to the central idea, or thesis, of the paper (Rosen and Behrens 119).\n",
    "Coherent: The sentences should be arranged in a logical manner and should follow a definite plan for development (Rosen and Behrens 119).\n",
    "Well-developed: Every idea discussed in the paragraph should be adequately explained and supported through evidence and details that work together to explain the paragraph’s controlling idea (Rosen and Behrens 119).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b940c40-4567-4943-99b2-1c650e1fbf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "408c3771-0787-45ec-ac29-15d69d2902f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2c2ab37-1a28-4651-9c15-682e544df3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = [word for word in text_split if word not in stopwords and str(word).isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6597a83-699b-4310-be51-37990e524fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "li = [str(word) for word in cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94923611-f19c-4975-9573-344f09419535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Before',\n",
       " 'you',\n",
       " 'can',\n",
       " 'begin',\n",
       " 'to',\n",
       " 'determine',\n",
       " 'what',\n",
       " 'the',\n",
       " 'composition',\n",
       " 'of',\n",
       " 'a',\n",
       " 'particular',\n",
       " 'paragraph',\n",
       " 'will',\n",
       " 'be',\n",
       " 'you',\n",
       " 'must',\n",
       " 'first',\n",
       " 'decide',\n",
       " 'on',\n",
       " 'an',\n",
       " 'argument',\n",
       " 'and',\n",
       " 'a',\n",
       " 'working',\n",
       " 'thesis',\n",
       " 'statement',\n",
       " 'for',\n",
       " 'your',\n",
       " 'paper',\n",
       " 'What',\n",
       " 'is',\n",
       " 'the',\n",
       " 'most',\n",
       " 'important',\n",
       " 'idea',\n",
       " 'that',\n",
       " 'you',\n",
       " 'are',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'convey',\n",
       " 'to',\n",
       " 'your',\n",
       " 'reader',\n",
       " 'The',\n",
       " 'information',\n",
       " 'in',\n",
       " 'each',\n",
       " 'paragraph',\n",
       " 'must',\n",
       " 'be',\n",
       " 'related',\n",
       " 'to',\n",
       " 'that',\n",
       " 'idea',\n",
       " 'In',\n",
       " 'other',\n",
       " 'words',\n",
       " 'your',\n",
       " 'paragraphs',\n",
       " 'should',\n",
       " 'remind',\n",
       " 'your',\n",
       " 'reader',\n",
       " 'that',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'recurrent',\n",
       " 'relationship',\n",
       " 'between',\n",
       " 'your',\n",
       " 'thesis',\n",
       " 'and',\n",
       " 'the',\n",
       " 'information',\n",
       " 'in',\n",
       " 'each',\n",
       " 'paragraph',\n",
       " 'A',\n",
       " 'working',\n",
       " 'thesis',\n",
       " 'functions',\n",
       " 'like',\n",
       " 'a',\n",
       " 'seed',\n",
       " 'from',\n",
       " 'which',\n",
       " 'your',\n",
       " 'paper',\n",
       " 'and',\n",
       " 'your',\n",
       " 'ideas',\n",
       " 'will',\n",
       " 'grow',\n",
       " 'The',\n",
       " 'whole',\n",
       " 'process',\n",
       " 'is',\n",
       " 'an',\n",
       " 'organic',\n",
       " 'one',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'progression',\n",
       " 'from',\n",
       " 'a',\n",
       " 'seed',\n",
       " 'to',\n",
       " 'a',\n",
       " 'full',\n",
       " 'blown',\n",
       " 'paper',\n",
       " 'where',\n",
       " 'there',\n",
       " 'are',\n",
       " 'direct',\n",
       " 'familial',\n",
       " 'relationships',\n",
       " 'between',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ideas',\n",
       " 'in',\n",
       " 'the',\n",
       " 'paper',\n",
       " 'The',\n",
       " 'decision',\n",
       " 'about',\n",
       " 'what',\n",
       " 'to',\n",
       " 'put',\n",
       " 'into',\n",
       " 'your',\n",
       " 'paragraphs',\n",
       " 'begins',\n",
       " 'with',\n",
       " 'the',\n",
       " 'germination',\n",
       " 'of',\n",
       " 'a',\n",
       " 'seed',\n",
       " 'of',\n",
       " 'ideas',\n",
       " 'this',\n",
       " 'germination',\n",
       " 'process',\n",
       " 'is',\n",
       " 'better',\n",
       " 'known',\n",
       " 'as',\n",
       " 'brainstorming',\n",
       " 'There',\n",
       " 'are',\n",
       " 'many',\n",
       " 'techniques',\n",
       " 'for',\n",
       " 'brainstorming',\n",
       " 'whichever',\n",
       " 'one',\n",
       " 'you',\n",
       " 'choose',\n",
       " 'this',\n",
       " 'stage',\n",
       " 'of',\n",
       " 'paragraph',\n",
       " 'development',\n",
       " 'can',\n",
       " 'not',\n",
       " 'be',\n",
       " 'skipped',\n",
       " 'Building',\n",
       " 'paragraphs',\n",
       " 'can',\n",
       " 'be',\n",
       " 'like',\n",
       " 'building',\n",
       " 'a',\n",
       " 'skyscraper',\n",
       " 'there',\n",
       " 'must',\n",
       " 'be',\n",
       " 'a',\n",
       " 'well',\n",
       " 'planned',\n",
       " 'foundation',\n",
       " 'that',\n",
       " 'supports',\n",
       " 'what',\n",
       " 'you',\n",
       " 'are',\n",
       " 'building',\n",
       " 'Any',\n",
       " 'cracks',\n",
       " 'inconsistencies',\n",
       " 'or',\n",
       " 'other',\n",
       " 'corruptions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'foundation',\n",
       " 'can',\n",
       " 'cause',\n",
       " 'your',\n",
       " 'whole',\n",
       " 'paper',\n",
       " 'to',\n",
       " 'crumble',\n",
       " 'So',\n",
       " 'let',\n",
       " 'suppose',\n",
       " 'that',\n",
       " 'you',\n",
       " 'have',\n",
       " 'done',\n",
       " 'some',\n",
       " 'brainstorming',\n",
       " 'to',\n",
       " 'develop',\n",
       " 'your',\n",
       " 'thesis',\n",
       " 'What',\n",
       " 'else',\n",
       " 'should',\n",
       " 'you',\n",
       " 'keep',\n",
       " 'in',\n",
       " 'mind',\n",
       " 'as',\n",
       " 'you',\n",
       " 'begin',\n",
       " 'to',\n",
       " 'create',\n",
       " 'paragraphs',\n",
       " 'Every',\n",
       " 'paragraph',\n",
       " 'in',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'should',\n",
       " 'be',\n",
       " 'Unified',\n",
       " 'All',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sentences',\n",
       " 'in',\n",
       " 'a',\n",
       " 'single',\n",
       " 'paragraph',\n",
       " 'should',\n",
       " 'be',\n",
       " 'related',\n",
       " 'to',\n",
       " 'a',\n",
       " 'single',\n",
       " 'controlling',\n",
       " 'idea',\n",
       " 'often',\n",
       " 'expressed',\n",
       " 'in',\n",
       " 'the',\n",
       " 'topic',\n",
       " 'sentence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'paragraph',\n",
       " 'Clearly',\n",
       " 'related',\n",
       " 'to',\n",
       " 'the',\n",
       " 'thesis',\n",
       " 'The',\n",
       " 'sentences',\n",
       " 'should',\n",
       " 'all',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " 'central',\n",
       " 'idea',\n",
       " 'or',\n",
       " 'thesis',\n",
       " 'of',\n",
       " 'the',\n",
       " 'paper',\n",
       " 'Rosen',\n",
       " 'and',\n",
       " 'Behrens',\n",
       " '119',\n",
       " 'Coherent',\n",
       " 'The',\n",
       " 'sentences',\n",
       " 'should',\n",
       " 'be',\n",
       " 'arranged',\n",
       " 'in',\n",
       " 'a',\n",
       " 'logical',\n",
       " 'manner',\n",
       " 'and',\n",
       " 'should',\n",
       " 'follow',\n",
       " 'a',\n",
       " 'definite',\n",
       " 'plan',\n",
       " 'for',\n",
       " 'development',\n",
       " 'Rosen',\n",
       " 'and',\n",
       " 'Behrens',\n",
       " '119',\n",
       " 'Well',\n",
       " 'developed',\n",
       " 'Every',\n",
       " 'idea',\n",
       " 'discussed',\n",
       " 'in',\n",
       " 'the',\n",
       " 'paragraph',\n",
       " 'should',\n",
       " 'be',\n",
       " 'adequately',\n",
       " 'explained',\n",
       " 'and',\n",
       " 'supported',\n",
       " 'through',\n",
       " 'evidence',\n",
       " 'and',\n",
       " 'details',\n",
       " 'that',\n",
       " 'work',\n",
       " 'together',\n",
       " 'to',\n",
       " 'explain',\n",
       " 'the',\n",
       " 'paragraph',\n",
       " 'controlling',\n",
       " 'idea',\n",
       " 'Rosen',\n",
       " 'and',\n",
       " 'Behrens',\n",
       " '119']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1e1ed23-cc9a-47ff-a1e3-6ea86cf11755",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned2 = ' '.join(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fd78f8b-54c7-4c37-870c-f0bb3ca807ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned2 = list(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f3e20-44d9-47df-a7b2-86086cf759a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b4923-660a-43d5-83a6-c42ce3ef9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ce5fa-0f0c-4d7b-b488-a09321c1dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = WordCloud(width=1200,height = 1200).generate(str(cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226621a1-ff72-4f8a-be66-4a9719a63655",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(x)\n",
    "\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c7e7c4-d8e2-4618-af59-a976af630c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "clinical_data = [\n",
    "    \"Patient ID: 001, Age: 45, Diagnosis: Diabetes\",\n",
    "    \"Patient ID: 002, Age: 60, Diagnosis: Hypertension\",\n",
    "    \"Patient ID: 003, Age: 29, Diagnosis: Diabetes\",\n",
    "    \"Patient ID: 004, Age: 40, Diagnosis: BloodPressure\",\n",
    "    \"Patient ID: 005, Age: 52, Diagnosis: Depression\"\n",
    "]\n",
    "\n",
    "x = [re.findall(r'Patient ID: (\\d+), Age: (\\d+), Diagnosis: (\\w+)', record) for record in clinical_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39941aaa-6d7b-4efb-a830-eb77610c1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list()\n",
    "for i in x:\n",
    "    for j in i:\n",
    "        l.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef4fa58-3f1b-46a3-bfc7-88b0227af362",
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2af45b-4edf-4c67-a5d3-369ff88478cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = pd.DataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd822d-a6b5-478a-8107-8d8811803b24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y.columns = ['A','B','B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a564b32b-3677-4622-9254-d742a408add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60e09daa-ab39-47e3-8e7e-1573deb8cd33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Before',\n",
       " 'you',\n",
       " 'can',\n",
       " 'begin',\n",
       " 'to',\n",
       " 'determine',\n",
       " 'what',\n",
       " 'the',\n",
       " 'composition',\n",
       " 'of',\n",
       " 'a',\n",
       " 'particular',\n",
       " 'paragraph',\n",
       " 'will',\n",
       " 'be',\n",
       " 'you',\n",
       " 'must',\n",
       " 'first',\n",
       " 'decide',\n",
       " 'on']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned2 = cleaned2[:20]\n",
    "cleaned2 = [str(i) for i in cleaned2]\n",
    "cleaned2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a68dc5b0-3704-457b-8eba-f9e56b43e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9563d6c6-d3eb-4c11-858e-9374b1b941a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = TfidfVectorizer(max_features = 10)\n",
    "words = x.fit_transform(cleaned2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3cf62b11-96a1-4db4-b398-768f55fda5b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['be', 'first', 'of', 'on', 'paragraph', 'particular', 'the', 'to',\n",
       "       'what', 'you'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0779e7b-44d8-4666-beae-b0a4452efd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = words.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e824efef-34da-4cda-bd7d-a0345555f37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05241c1d-e800-43c8-ad77-46d8a343e563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blood: 0.3333\n",
      "confirmed: 0.3333\n",
      "diagnosis: 0.3333\n",
      "high: 0.3333\n",
      "history: 0.3333\n",
      "hypertension: 0.3333\n",
      "patient: 0.3333\n",
      "prescribed: 0.3333\n",
      "tests: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sailesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample paragraph\n",
    "paragraph = \"\"\"The patient has a history of hypertension and was prescribed medication.\n",
    "               The diagnosis confirmed high blood pressure. Further tests recommended.\"\"\"\n",
    "\n",
    "# Step 1: Clean the paragraph\n",
    "def clean_text(text):\n",
    "    words = text.lower().split()\n",
    "    cleaned = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_text(paragraph)\n",
    "\n",
    "# Step 2: Apply TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=10)\n",
    "tfidf_matrix = vectorizer.fit_transform([cleaned_text])  # Pass as a list of strings\n",
    "\n",
    "# Step 3: View TF-IDF values\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "scores = tfidf_matrix.toarray()[0]\n",
    "\n",
    "for word, score in zip(feature_names, scores):\n",
    "    print(f\"{word}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad66fa6-5dfa-4064-9f68-0fd3fe95c9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
